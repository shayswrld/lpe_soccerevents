{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "class callback(CallbackAny2Vec):\n",
    "    '''Callback to print loss after each epoch.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        prev = 0\n",
    "        loss = model.get_latest_training_loss()\n",
    "        change_loss = loss - prev\n",
    "        prev = loss\n",
    "        print('Loss after epoch {}: {}'.format(self.epoch, change_loss))\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the corpus, cleaning for punctuation, tags and multiple whitespaces\n",
    "Shuffe the corpus\n",
    "Yield each sentence of the shuffled corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "import gensim.parsing.preprocessing as gsp\n",
    "import random\n",
    "\n",
    "class EventCorpus:\n",
    "\t\"\"\"An iterator that yields sentences (lists of str) for pseudo event corpus.\"\"\"\n",
    "\tdef __iter__(self):\n",
    "\t\tsentences = []\n",
    "\t\tcorpus_path = datapath('/home/vlnsha004/CSC2005Z/lpe_soccerevents/data/eventcorpus8.txt')\n",
    "\t\t\n",
    "\t\tfor line in open(corpus_path):\n",
    "\n",
    "\t\t\t# remove all single quotation marks\n",
    "\t\t\tline = line.replace(\"'\", \"\") # Erroneous quotation marks\n",
    "\t\t\tline = line.replace('6', '5')\n",
    "\t\t\tline = line.replace('\"', '')\n",
    "\t\t\tCUSTOM_FILTERS = [gsp.strip_tags, gsp.strip_multiple_whitespaces]\n",
    "\t\t\tsentences.append(gsp.preprocess_string(line, CUSTOM_FILTERS))\n",
    "\t\t\tyield gsp.preprocess_string(line, CUSTOM_FILTERS)\n",
    "        \n",
    "\t\trandom.shuffle(sentences)  # Shuffle the sentences\n",
    "\n",
    "\t\tfor sentence in sentences:\n",
    "\t\t\tyield sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training\n",
    "Current hyperparameters:\n",
    "window = 3\n",
    "epochs = 35\n",
    "vs = 500\n",
    "mc = 1 # min_count\n",
    "negative = 20, Negative used for the sample size used for negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train the model on the new data\n",
    "# model.train(sequences, total_examples=model.corpus_count, epochs=model.epochs, compute_loss = True, callbacks=[callback()])\n",
    "\n",
    "sequences = EventCorpus()\n",
    "\n",
    "window = 10\n",
    "epochs = 60\n",
    "vs = 800\n",
    "mc = 1 # min_count\n",
    "ns = 20 # negative sampling\n",
    "subsample = 1e-5\n",
    "model = Word2Vec(sentences=sequences, epochs = epochs, sample=subsample, vector_size=vs, window=window, min_count=mc, workers=10, compute_loss = True, negative=ns, callbacks=[callback()]) #Obtain params from football2vec text\n",
    "model.save(f\"/home/vlnsha004/CSC2005Z/lpe_soccerevents/models/anon/Seq2vec_{epochs}_{vs}_{window}_{mc}_{ns}_shuffled_sample.model\") # First model trained on full corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = EventCorpus()\n",
    "window = 3\n",
    "epochs = 30\n",
    "vs = 500\n",
    "mc = 1 # min_count\n",
    "ns = 5 # negative sampling\n",
    "for i in range(6, 18, 6):\n",
    "    window = i\n",
    "    for j in range(5, 25, 15):\n",
    "        ns = j\n",
    "        model = Word2Vec(sentences=sequences, epochs = epochs, vector_size=vs, window=window, min_count=mc, workers=8, compute_loss = True, negative=ns, callbacks=[callback()])\n",
    "        model.save(f\"/home/vlnsha004/CSC2005Z/player2vec/models/anon/Seq2vec_{epochs}_{vs}_{window}_{mc}_{ns}_unshuffled.model\") # First model trained on full corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"/home/vlnsha004/CSC2005Z/lpe_soccerevents/models/anon/Seq2vec_{epochs}_{vs}_{window}_{mc}_{ns}_unshuffled.model\") # First model trained on full corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model_path = \"/home/vlnsha004/CSC2005Z/lpe_soccerevents/models/anon/Seq2vec_30_500_3_1_20_shuffled.model\"\n",
    "model = Word2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fifa =  pd.read_csv('/home/vlnsha004/CSC2005Z/lpe_soccerevents/data/male_players.csv', delimiter=',', low_memory=False)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import multiprocessing as mp\n",
    "\n",
    "fifa = fifa.query('fifa_version > 23.0')\n",
    "fifa = fifa.sort_values(['overall', 'player_positions'], ascending = False)\n",
    "fifa['long_name'] = fifa['long_name'].str.replace(' ', '_')\n",
    "\n",
    "fifa = fifa.reset_index(drop=True, inplace=False) # reset the indices\n",
    "fifa = fifa.rename_axis(\"index\", axis=\"columns\")\n",
    "trimmed_fifa = fifa[[\"long_name\", \"player_positions\", \"player_tags\", \"player_traits\"]]\n",
    "take_sample_400 = trimmed_fifa.sample(n=400, random_state=1, replace=False)\n",
    "top400 = dd.from_pandas(take_sample_400, npartitions=mp.cpu_count()) #Take 400 of the top rated players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(baller, sim_player, pd_fifa, trials, type) -> int:\n",
    "\t'''type -> compare positions or traits or tags'''\n",
    "\t'''type: string representing column name '\"player_positions\", \"player_tags\", \"player_traits\"'''\n",
    "\n",
    "\tballer_positions = (pd_fifa.loc[pd_fifa['long_name'] == baller, type]).unique().tolist()\n",
    "\tif len(baller_positions) == 0 or str(baller_positions[0]) == 'nan':\n",
    "\t\tprint(f'{baller} has no player tags in FIFA\\n')\n",
    "\t\ttrials = 1\n",
    "\t\tprint(f'Trials = {trials}\\n')\n",
    "\t\treturn [0, trials]\n",
    "\tballer_set = set(baller_positions[0].split(', '))\n",
    "\t# Return 1 if sim_player plays any of the baller's positions, 0 otherwise\n",
    "\t# Find rows where sim_player is a subset of long_name efficiently with vectorization\n",
    "\tsubset_mask = pd_fifa['long_name'].str.contains(sim_player[0], case=False, regex=False)  # Case-insensitive\n",
    "\t# Assuming there's only one matching record (use .iloc[0] for the first)\n",
    "\tmatching_row = pd_fifa[subset_mask]\n",
    "\tretrieval_w2v_positions = matching_row[type].unique().tolist()\n",
    "\tif len(retrieval_w2v_positions) == 0 or 'nan' in (str(retrieval_w2v_positions)):\n",
    "\t\tprint(f'{sim_player[0]} not found or has no traits in FIFA\\n')\n",
    "\t\tprint(f'Trials = {trials}\\n')\n",
    "\t\treturn [0, trials]\n",
    "\telif len(retrieval_w2v_positions) > 1:\n",
    "\t\tprint(f'{sim_player[0]}: {retrieval_w2v_positions}')\n",
    "\t\tretrieval_set = set(retrieval_w2v_positions[1].split(', '))\n",
    "\t\tprint(f'{baller}: {baller_positions}')    \n",
    "\t\tprint(f'similar player set: {retrieval_set}')\n",
    "\t\tprint(f'baller set: {baller_set}')\n",
    "\t\tprint(f'intersection: {retrieval_set & baller_set}')\n",
    "\t\tif (retrieval_set & baller_set):\n",
    "\t\t\t# print('SIMILAR PLAYER FOUND\\n')\n",
    "\t\t\ttrials += 1\n",
    "\t\t\treturn [1, trials]\n",
    "\t\telse:\n",
    "\t\t\t# print('No match\\n')\n",
    "\t\t\ttrials += 1\n",
    "\t\t\treturn [0, trials]\n",
    "\telse:\n",
    "\t\tprint(f'{sim_player[0]}: {retrieval_w2v_positions}')\n",
    "\t\tretrieval_set = set(retrieval_w2v_positions[0].split(', '))\n",
    "\t\tprint(f'{baller}: {baller_positions}')    \n",
    "\t\tprint(f'similar player set: {retrieval_set}')\n",
    "\t\tprint(f'baller set: {baller_set}')\n",
    "\t\tprint(f'intersection: {retrieval_set & baller_set}')\n",
    "\t\tif (retrieval_set & baller_set):\n",
    "\t\t\tprint('SIMILAR PLAYER FOUND\\n')\n",
    "\t\t\ttrials += 1\n",
    "\t\t\treturn [1, trials]\n",
    "\t\telse:\n",
    "\t\t\tprint('No match\\n')\n",
    "\t\t\ttrials += 1\n",
    "\t\t\treturn [0, trials]\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def get_pakvector(top400, base=False, type=\"player_positions\", model=model):\n",
    "\n",
    "\tsimilar_players = {}\n",
    "\n",
    "\tfor player in top400['long_name']:\n",
    "\t\t# Create dictionary with {\"Eriksen\": [\"Davies\", \"Mignolet\", \"Ronaldo\"], \"Johnathan\": [\"Bakambu\", \"Cedric\"]}\n",
    "\t\ttry: \n",
    "\t\t\tif base:\n",
    "\t\t\t\tall_players = model.wv.index_to_key\n",
    "\t\t\t\trandom_players = random.sample(all_players, 200)\n",
    "\t\t\t\tsimilar_players[player] = [(other_player, model.wv.similarity(player, other_player)) for other_player in random_players]\n",
    "\t\t\telse:\n",
    "\t\t\t\tsimilar_players[player] = model.wv.most_similar(player, topn=60)\n",
    "\t\texcept KeyError:\n",
    "\t\t\tcontinue\n",
    "\t\t\n",
    "\t# Create an array to store the precision at 20 for each player in selected_columns, retrieving similar players by the model\n",
    "\tpak_store = []\n",
    "\tfor baller, similar in similar_players.items(): # Loop through the key-value pairs\n",
    "\t\tcounter = 0\n",
    "\t\ttrials = 0\n",
    "\t\tprint(f'{baller}: {similar}')\n",
    "\t\tballer_positions = (trimmed_fifa.loc[trimmed_fifa['long_name'] == baller, type]).unique().tolist()\n",
    "\t\tif len(baller_positions) == 0 or 'nan' in (str(baller_positions)):\n",
    "\t\t\tprint(f'{baller} has no player tags in FIFA\\n')\n",
    "\t\t\tcontinue\n",
    "\t\t# Begin trials\n",
    "\t\tfor sim_player in similar: # For each player in similar set:\t\n",
    "\t\t\t[match, new_trial] = compare(baller, sim_player, trimmed_fifa, trials, type)\n",
    "\t\t\tcounter += match\n",
    "\t\t\ttrials = new_trial\n",
    "\t\t\tif trials == 10:\n",
    "\t\t\t\tprop = counter/trials\n",
    "\t\t\t\tbreak\n",
    "\t\t# If we do not have 10 trials, do not include in the precision at 10 calculation\n",
    "\t\t# Have gone through all retrievals and found no viable comparisons\n",
    "\t\tif trials < 10:\n",
    "\t\t\tprint(f'Less than 10 data points for similar players to {baller} in FIFA\\n')\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tprint(f'{baller}: Correct:{counter}, Trials:{trials}, Percent: {prop}\\n')\n",
    "\t\tpak_store.append(prop) # Store the precision at 20 for each player\n",
    "\tave_precision_k = round(np.mean(pak_store) * 100, 2)\n",
    "\treturn ave_precision_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_pakvector(top400, base=False, type=\"player_tags\", model = model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "directory = \"/home/vlnsha004/CSC2005Z/lpe_soccerevents/models/anon\"\n",
    "types = [\"player_positions\", \"player_tags\", \"player_traits\"]\n",
    "models = {}\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".model\"):\n",
    "        model_path = os.path.join(directory, filename)\n",
    "        model_name = filename[:-6]  # remove \".model\" from the filename\n",
    "        models[model_name] = Word2Vec.load(model_path)\n",
    "        \n",
    "\n",
    "latex_table = r\"\"\"\n",
    "\\begin{tabular}{|l||c|c||c|c|c||c|}\n",
    "\\hline\n",
    "Model    &   Negative          & Anonymized/    & Shuffled/    & \\multicolumn{3}{c||}{Accuracy [\\%]} \\\\\n",
    "         &   Sample  & Unanonymized   & Unshuffled   & \\multicolumn{3}{c||}{Precision at K} \\\\\n",
    "\\hline\n",
    "         &                             &       &    & Player Positions      &  Player Tags    &  Player Traits               \\\\\n",
    "\\hline\n",
    "\"\"\"\n",
    "\n",
    "# Add the random/base set of results\n",
    "latex_table += f\"Randomly drawn set of retrievals & N/A & N/A & N/A & \"\n",
    "for type in types:\n",
    "    pak = get_pakvector(top400, base = True, type=type)\n",
    "    latex_table += f\"{pak} & \"\n",
    "latex_table = latex_table[:-2]  # remove the last \"& \"\n",
    "latex_table += r\"\\\\ \\hline\"  # add a new row\n",
    "\n",
    "# Define the sorting key\n",
    "def sorting_key(item):\n",
    "    model_name, model = item\n",
    "    shuffled = \"shuffled\" in model_name\n",
    "    return (shuffled, model.negative)\n",
    "\n",
    "# Sort the models\n",
    "sorted_models = sorted(models.items(), key=sorting_key)\n",
    "\n",
    "for model_name, model in sorted_models:\n",
    "    anonymized = \"Anon\" if \"anon\" in model_name else \"Unanon\"\n",
    "    shuffled = \"Shuffled\" if \"shuffled\" in model_name else \"Unshuffled\"\n",
    "    latex_table += f\"{model_name} & {model.negative} & {anonymized} & {shuffled} & \"\n",
    "    for type in types:\n",
    "        pak = get_pakvector(top400, type=type, model = model)\n",
    "        latex_table += f\"{pak} & \"\n",
    "    latex_table = latex_table[:-2]  # remove the last \"& \"\n",
    "    latex_table += r\"\\\\ + \\n+ \\hline + \\n\"  # add a new row\n",
    "\n",
    "latex_table += r\"\\end{tabular}\"\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation and retrievals:\n",
    "\n",
    "word = 'Anderson_Luís_de_Souza'\n",
    "try:\n",
    "\tprint(model.wv.most_similar(word, topn = 45, ))\n",
    "except KeyError:\n",
    "    print(f\"The word {word} does not appear in current model\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    ('Diego_Armando_Maradona', word),  # a minivan is a kind of car\n",
    "    ('long_pass', word),   # a minivan is a kind of car\n",
    "    ('High_Pass', word),   # still a wheeled vehicle\n",
    "    ('short_pass', word),  # ok, no wheels, but still a vehicle\n",
    "    ('Ronaldo_de_Assis_Moreira', word),    # ... and so on\n",
    "    ('Shot_Goal', word),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print(f'{w1}\\t{w2}\\t{model.wv.similarity(w1, w2)}')\n",
    "    # print(f'{w1}\\t{w2}\\t{model300.wv.similarity(w1.lower(), w2.lower())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embeddings for three words\n",
    "word2_vector = model.wv['Kylian_Mbappé_Lottin']\n",
    "word3_vector = model.wv['Low_Pass']\n",
    "word1_vector = model.wv['Interception']\n",
    "word4_vector = model.wv['(4,4)']\n",
    "\n",
    "# Perform vector operations\n",
    "result_vector = word1_vector + word3_vector + word2_vector\n",
    "# Get the words most similar to the result vector\n",
    "most_similar_words = model.wv.most_similar(positive=[result_vector])\n",
    "\n",
    "# Print the most similar words\n",
    "for word, similarity in most_similar_words:\n",
    "\tprint(word, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vocab of model\n",
    "\n",
    "w2v_vocabulary = model.wv.key_to_index\n",
    "print((w2v_vocabulary))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
